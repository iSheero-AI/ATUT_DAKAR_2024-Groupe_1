{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb898782-4d1d-4726-90aa-f0afcbdc22ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cabbf60a-82ec-4b18-817f-243d499070dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Le répertoire 'scratch' existe déjà. Aucune action supplémentaire requise.\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import shutil\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "from docling.document_converter import DocumentConverter\n",
    "#from google.colab import files\n",
    "\n",
    "# 1. Fonction pour récupérer les chemins d'accès aux fichiers\n",
    "def obtenir_chemins_acces(chemin_acces, extensions=None):\n",
    "    return [\n",
    "        os.path.join(root, file)\n",
    "        for root, _, files in os.walk(chemin_acces)\n",
    "        for file in files\n",
    "        if not extensions or file.lower().endswith(tuple(extensions))\n",
    "    ]\n",
    "\n",
    "# 2. Filtrer les chemins par format\n",
    "def filtrer_par_format(chemins, formats):\n",
    "    return {fmt: [path for path in chemins if path.lower().endswith(f\".{fmt}\")] for fmt in formats}\n",
    "\n",
    "# 3. Charger les fichiers\n",
    "def charger_fichier_txt(chemins, output_dir):\n",
    "    for path in chemins:\n",
    "        try:\n",
    "            with open(path, 'r', encoding='utf-8') as source_file:\n",
    "                filename = os.path.basename(path)\n",
    "                dest_path = output_dir / filename\n",
    "                with open(dest_path, 'w', encoding='utf-8') as dest_file:\n",
    "                    dest_file.write(source_file.read())\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du traitement du fichier TXT {path}: {e}\")\n",
    "\n",
    "def charger_fichier_csv(chemins, output_dir):\n",
    "    for path in chemins:\n",
    "        try:\n",
    "            filename = os.path.basename(path).replace('.csv', '.txt')\n",
    "            dest_path = output_dir / filename\n",
    "            with open(path, 'r') as fichier_csv, open(dest_path, 'w') as fichier_txt:\n",
    "                lecteur_csv = csv.reader(fichier_csv)\n",
    "                for ligne in lecteur_csv:\n",
    "                    fichier_txt.write('\\t'.join(ligne) + '\\n')\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du traitement du fichier CSV {path}: {e}\")\n",
    "\n",
    "def charger_fichier_pdf_html(chemins, output_dir, format_type):\n",
    "    for path in chemins:\n",
    "        try:\n",
    "            filename = os.path.basename(path)\n",
    "            converter = DocumentConverter()\n",
    "            result = converter.convert(path)\n",
    "            dest_path = output_dir / f\"{filename}.md\"\n",
    "            with dest_path.open(\"w\", encoding=\"utf-8\") as fp:\n",
    "                fp.write(result.document.export_to_markdown())\n",
    "        except Exception as e:\n",
    "            print(f\"Erreur lors du traitement du fichier {format_type.upper()} {path}: {e}\")\n",
    "\n",
    "# 4. Ajouter .txt aux fichiers Markdown\n",
    "def ajouter_extension_txt(output_dir):\n",
    "    for filename in output_dir.iterdir():\n",
    "        if filename.suffix == '.md':\n",
    "            new_filename = filename.with_suffix('.md.txt')\n",
    "            filename.rename(new_filename)\n",
    "            print(f\"Renommé: {filename} -> {new_filename}\")\n",
    "\n",
    "# 5. Archiver et télécharger le répertoire\n",
    "def zipper_et_telecharger(output_dir, zip_name=\"scratch\"):\n",
    "    zip_path = f\"{zip_name}.zip\"\n",
    "    if not os.path.exists(zip_path):  # Ne compresse et ne télécharge que si le fichier n'existe pas\n",
    "        shutil.make_archive(zip_name, 'zip', str(output_dir))\n",
    "        files.download(zip_path)\n",
    "    else:\n",
    "        print(f\"Le fichier compressé '{zip_path}' existe déjà. Aucun téléchargement nécessaire.\")\n",
    "\n",
    "# Pipeline principale\n",
    "def pipeline(chemin_acces, formats, output_dir):\n",
    "    # Vérification : ne pas recréer ou recharger si le dossier existe déjà\n",
    "    if output_dir.exists():\n",
    "        print(f\"Le répertoire '{output_dir}' existe déjà. Aucune action supplémentaire requise.\")\n",
    "        return\n",
    "\n",
    "    # Préparation\n",
    "    chemins = obtenir_chemins_acces(chemin_acces)\n",
    "    fichiers_par_format = filtrer_par_format(chemins, formats)\n",
    "    \n",
    "    # Créer le dossier de sortie\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    # Charger les fichiers\n",
    "    charger_fichier_txt(fichiers_par_format['txt'], output_dir)\n",
    "    charger_fichier_csv(fichiers_par_format['csv'], output_dir)\n",
    "    charger_fichier_pdf_html(fichiers_par_format['pdf'], output_dir, \"pdf\")\n",
    "    charger_fichier_pdf_html(fichiers_par_format['html'], output_dir, \"html\")\n",
    "    \n",
    "    # Ajouter extension .txt et compresser\n",
    "    ajouter_extension_txt(output_dir)\n",
    "    zipper_et_telecharger(output_dir)\n",
    "\n",
    "# Exécution\n",
    "if __name__ == \"__main__\":\n",
    "    chemin_acces = os.getcwd()+\"/data\"  # À remplacer\n",
    "    formats_supportés = [\"txt\", \"csv\", \"pdf\", \"html\"]\n",
    "    output_dir = Path(\"scratch\")\n",
    "    \n",
    "    pipeline(chemin_acces, formats_supportés, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b2b11c78-2ce2-4d1d-b8e0-7fe3a85c4b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\LENOVO\\\\Desktop\\\\Equipe 1\\\\ATUT_DAKAR_2024-Groupe_1'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d7c8066-56b3-4c4d-954d-822fc9682233",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name '_name_' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 50\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m all_chunks\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# Exemple d'utilisation\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43m_name_\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_main_\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     51\u001b[0m     folder_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mscratch\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     52\u001b[0m     processor \u001b[38;5;241m=\u001b[39m DocumentProcessor(chunk_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1024\u001b[39m, chunk_overlap\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name '_name_' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "class DocumentProcessor:\n",
    "    def __init__(self, chunk_size=1024, chunk_overlap=50):\n",
    "        self.chunk_size = chunk_size\n",
    "        self.chunk_overlap = chunk_overlap\n",
    "\n",
    "\n",
    "\n",
    "    def split_text(self, text):\n",
    "     #Découper le texte en chunks\n",
    "        chunks = []\n",
    "        start = 0\n",
    "        while start < len(text):\n",
    "            end = min(start + self.chunk_size, len(text))\n",
    "            chunks.append(text[start:end])\n",
    "            start += self.chunk_size - self.chunk_overlap  # Décalage avec chevauchement\n",
    "        return chunks\n",
    "\n",
    "    def process_file(self, file_path):\n",
    "        \"\"\"\n",
    "        Lit un fichier, le découpe en chunks, et ajoute des métadonnées.\n",
    "        \"\"\"\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "\n",
    "        # Créer des métadonnées\n",
    "        metadata = {\n",
    "            \"document_name\": os.path.basename(file_path),\n",
    "            \"path\": file_path,\n",
    "            \"source_type\": \"txt\"\n",
    "        }\n",
    "\n",
    "        # Découper le texte et associer les métadonnées\n",
    "        chunks = self.split_text(text)\n",
    "        processed_chunks = [{\"content\": chunk, \"metadata\": metadata} for chunk in chunks]\n",
    "\n",
    "        return processed_chunks\n",
    "\n",
    "    def process_folder(self, folder_path):\n",
    "        \"\"\"\n",
    "        Parcourt un dossier, traite chaque fichier .txt, et renvoie une liste de chunks.\n",
    "        \"\"\"\n",
    "        all_chunks = []\n",
    "        for filename in os.listdir(folder_path):\n",
    "            if filename.endswith(\".txt\"):\n",
    "                file_path = os.path.join(folder_path, filename)\n",
    "                chunks = self.process_file(file_path)\n",
    "                all_chunks.extend(chunks)  # Ajouter tous les chunks à la liste globale\n",
    "        return all_chunks\n",
    "\n",
    "\n",
    "# Exemple d'utilisation\n",
    "if __name__ == \"__main__\":\n",
    "    folder_path = \"/content/scratch\"\n",
    "    processor = DocumentProcessor()\n",
    "\n",
    "    # Traiter tous les fichiers dans le dossier\n",
    "    chunks = processor.process_folder(folder_path)\n",
    "\n",
    "    # Afficher les résultats\n",
    "    #print(f\"Nombre total de chunks : {len(chunks)}\")\n",
    "    #for i, chunk in enumerate(chunks[:3]):\n",
    "    #    print(f\"Chunk {i + 1}:\")\n",
    "    #    print(f\"Content: {chunk['content']}\")\n",
    "    #    print(f\"Metadata: {chunk['metadata']}\")\n",
    "    #    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1226d00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, using CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c24479",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model = SentenceTransformer('dangvantuan/french-document-embedding', trust_remote_code=True, device=device)\n",
    "embeddings = []\n",
    "for chunk in chunks:\n",
    "  sentences = chunk['content']\n",
    "  embedding = model.encode(sentences,device=device) # Use GPU for encoding)\n",
    "  embeddings.append(embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f97e78d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convertir les embeddings en NumPy float32 pour FAISS\n",
    "import numpy as np\n",
    "embeddings = np.array(embeddings, dtype=\"float32\")\n",
    "\n",
    "import faiss\n",
    "\n",
    "# Définir la dimension des vecteurs\n",
    "dimension = embeddings.shape[1]\n",
    "\n",
    "# Créer un index FAISS avec distance L2\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Ajouter les embeddings dans l'index\n",
    "index.add(embeddings)\n",
    "\n",
    "# Exemple de métadonnées associées à chaque chunk\n",
    "#metadata = [{\"id\": i, \"content\": chunk} for i, chunk in enumerate(chunks)]\n",
    "metadata = [{\"id\": i, \"content\": chunk['content'], \"metadata\": chunk[\"metadata\"]} for i, chunk in enumerate(chunks)]\n",
    "#metadata = [item[\"metadata\"] for item in chunks]\n",
    "\n",
    "# Sauvegarder les métadonnées parallèlement\n",
    "metadata_store = {i: meta for i, meta in enumerate(metadata)}\n",
    "\n",
    "print(f\"Nombre total de vecteurs dans l'index : {index.ntotal}\")\n",
    "\n",
    "# Associer les indices aux chunks\n",
    "id_to_chunk = {i: chunk for i, chunk in enumerate(chunks)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d447b1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "\n",
    "vector_store = FAISS(\n",
    "    embedding_function=None,\n",
    "    index=index,\n",
    "    docstore=chunks,\n",
    "    index_to_docstore_id=id_to_chunk\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee3f5285",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Effectuer une recherche\n",
    "\n",
    "query_vector = \"quel sont les codes de travail en benin\"\n",
    "\n",
    "# Convert the query to an embedding using the SentenceTransformer model\n",
    "query_embedding = model.encode(query_vector)\n",
    "\n",
    "# Reshape to a 2D array with a single row\n",
    "query_embedding = query_embedding.reshape(1, -1).astype(\"float32\")\n",
    "\n",
    "# Use query_embedding instead of query_vector for search\n",
    "distances, indices = index.search(query_embedding, 1)  # Trouver le plus proche\n",
    "\n",
    "# Get the index of the closest chunk\n",
    "closest_chunk_index = indices[0][0]\n",
    "\n",
    "# Access the metadata using the index from metadata_store\n",
    "result = metadata_store[closest_chunk_index]\n",
    "\n",
    "# Afficher les résultats\n",
    "print(f\"Résultat :\")\n",
    "print(f\"ID : {result['id']}\") # Now accessible through metadata_store\n",
    "print(f\"Contenu : {result['content']}\")\n",
    "print(f\"Metadata : {result['metadata']}\")\n",
    "print(f\"Distance : {distances[0][0]} \\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
