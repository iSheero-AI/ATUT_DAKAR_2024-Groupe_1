{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-TAkCrovhZw"
      },
      "outputs": [],
      "source": [
        "#!/usr/bin/env python\n",
        "import os\n",
        "from pathlib import Path\n",
        "import csv\n",
        "import shutil\n",
        "\n",
        "from langchain.document_loaders import TextLoader\n",
        "from docling.document_converter import DocumentConverter\n",
        "# from google.colab import files\n",
        "\n",
        "class LoadingExtraction:\n",
        "    def __init__(self, chemin_acces, formats, output_dir):\n",
        "        self.chemin_acces = chemin_acces\n",
        "        self.formats = formats\n",
        "        self.output_dir = Path(output_dir)\n",
        "\n",
        "    def obtenir_chemins_acces(self, extensions=None):\n",
        "        return [\n",
        "            os.path.join(root, file)\n",
        "            for root, _, files in os.walk(self.chemin_acces)\n",
        "            for file in files\n",
        "            if not extensions or file.lower().endswith(tuple(extensions))\n",
        "        ]\n",
        "\n",
        "    def filtrer_par_format(self, chemins):\n",
        "        return {fmt: [path for path in chemins if path.lower().endswith(f\".{fmt}\")] for fmt in self.formats}\n",
        "\n",
        "    def charger_fichier_txt(self, chemins):\n",
        "        for path in chemins:\n",
        "            try:\n",
        "                with open(path, 'r', encoding='utf-8') as source_file:\n",
        "                    filename = os.path.basename(path)\n",
        "                    dest_path = self.output_dir / filename\n",
        "                    with open(dest_path, 'w', encoding='utf-8') as dest_file:\n",
        "                        dest_file.write(source_file.read())\n",
        "            except Exception as e:\n",
        "                print(f\"Erreur lors du traitement du fichier TXT {path}: {e}\")\n",
        "\n",
        "    def charger_fichier_csv(self, chemins):\n",
        "        for path in chemins:\n",
        "            try:\n",
        "                filename = os.path.basename(path).replace('.csv', '.txt')\n",
        "                dest_path = self.output_dir / filename\n",
        "                with open(path, 'r') as fichier_csv, open(dest_path, 'w') as fichier_txt:\n",
        "                    lecteur_csv = csv.reader(fichier_csv)\n",
        "                    for ligne in lecteur_csv:\n",
        "                        fichier_txt.write('\\t'.join(ligne) + '\\n')\n",
        "            except Exception as e:\n",
        "                print(f\"Erreur lors du traitement du fichier CSV {path}: {e}\")\n",
        "\n",
        "    def charger_fichier_pdf_html(self, chemins, format_type):\n",
        "        for path in chemins:\n",
        "            try:\n",
        "                filename = os.path.basename(path)\n",
        "                converter = DocumentConverter()\n",
        "                result = converter.convert(path)\n",
        "                dest_path = self.output_dir / f\"{filename}.md\"\n",
        "                with dest_path.open(\"w\", encoding=\"utf-8\") as fp:\n",
        "                    fp.write(result.document.export_to_markdown())\n",
        "            except Exception as e:\n",
        "                print(f\"Erreur lors du traitement du fichier {format_type.upper()} {path}: {e}\")\n",
        "\n",
        "    def ajouter_extension_txt(self):\n",
        "        for filename in self.output_dir.iterdir():\n",
        "            if filename.suffix == '.md':\n",
        "                new_filename = filename.with_suffix('.md.txt')\n",
        "                filename.rename(new_filename)\n",
        "                print(f\"Renommé: {filename} -> {new_filename}\")\n",
        "\n",
        "    def zipper_et_telecharger(self, zip_name=\"scratch\"):\n",
        "        zip_path = f\"{zip_name}.zip\"\n",
        "        if not os.path.exists(zip_path):  # Ne compresse et ne télécharge que si le fichier n'existe pas\n",
        "            shutil.make_archive(zip_name, 'zip', str(self.output_dir))\n",
        "            # files.download(zip_path)\n",
        "        else:\n",
        "            print(f\"Le fichier compressé '{zip_path}' existe déjà. Aucun téléchargement nécessaire.\")\n",
        "\n",
        "    def pipeline(self):\n",
        "        if self.output_dir.exists():\n",
        "            print(f\"Le répertoire '{self.output_dir}' existe déjà. Aucune action supplémentaire requise.\")\n",
        "            return\n",
        "\n",
        "        # Préparation\n",
        "        chemins = self.obtenir_chemins_acces()\n",
        "        fichiers_par_format = self.filtrer_par_format(chemins)\n",
        "\n",
        "        # Créer le dossier de sortie\n",
        "        self.output_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "        # Charger les fichiers\n",
        "        self.charger_fichier_txt(fichiers_par_format.get('txt', []))\n",
        "        self.charger_fichier_csv(fichiers_par_format.get('csv', []))\n",
        "        self.charger_fichier_pdf_html(fichiers_par_format.get('pdf', []), \"pdf\")\n",
        "        self.charger_fichier_pdf_html(fichiers_par_format.get('html', []), \"html\")\n",
        "\n",
        "        # Ajouter extension .txt et compresser\n",
        "        self.ajouter_extension_txt()\n",
        "        self.zipper_et_telecharger()\n",
        "\n",
        "# Exécution\n",
        "if __name__ == \"__main__\":\n",
        "    chemin_acces = os.getcwd() + \"/data\"  # À remplacer\n",
        "    formats_supportés = [\"txt\", \"csv\", \"pdf\", \"html\"]\n",
        "    output_dir = \"scratch\"\n",
        "\n",
        "    loader = LoadingExtraction(chemin_acces, formats_supportés, output_dir)\n",
        "    loader.pipeline()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DocumentProcessor:\n",
        "    def __init__(self, chunk_size=1024, chunk_overlap=50):\n",
        "        self.chunk_size = chunk_size\n",
        "        self.chunk_overlap = chunk_overlap\n",
        "\n",
        "    def split_text(self, text):\n",
        "     #Découper le texte en chunks\n",
        "        chunks = []\n",
        "        start = 0\n",
        "        while start < len(text):\n",
        "            end = min(start + self.chunk_size, len(text))\n",
        "            chunks.append(text[start:end])\n",
        "            start += self.chunk_size - self.chunk_overlap  # Décalage avec chevauchement\n",
        "        return chunks\n",
        "\n",
        "    def process_file(self, file_path):\n",
        "        \"\"\"\n",
        "        Lit un fichier, le découpe en chunks, et ajoute des métadonnées.\n",
        "        \"\"\"\n",
        "        with open(file_path, 'r', encoding='utf-8') as file:\n",
        "            text = file.read()\n",
        "\n",
        "        # Créer des métadonnées\n",
        "        metadata = {\n",
        "            \"document_name\": os.path.basename(file_path),\n",
        "            \"path\": file_path,\n",
        "            \"source_type\": \"txt\"\n",
        "        }\n",
        "\n",
        "        # Découper le texte et associer les métadonnées\n",
        "        chunks = self.split_text(text)\n",
        "        processed_chunks = [{\"content\": chunk, \"metadata\": metadata} for chunk in chunks]\n",
        "\n",
        "        return processed_chunks\n",
        "\n",
        "    def process_folder(self, folder_path):\n",
        "        \"\"\"\n",
        "        Parcourt un dossier, traite chaque fichier .txt, et renvoie une liste de chunks.\n",
        "        \"\"\"\n",
        "        all_chunks = []\n",
        "        for filename in os.listdir(folder_path):\n",
        "            if filename.endswith(\".txt\"):\n",
        "                file_path = os.path.join(folder_path, filename)\n",
        "                chunks = self.process_file(file_path)\n",
        "                all_chunks.extend(chunks)  # Ajouter tous les chunks à la liste globale\n",
        "        return all_chunks\n",
        "\n",
        "\n",
        "# Exemple d'utilisation\n",
        "if __name__ == \"__main__\":\n",
        "    folder_path = \"/content/scratch\"\n",
        "    processor = DocumentProcessor()\n",
        "\n",
        "    # Traiter tous les fichiers dans le dossier\n",
        "    chunks = processor.process_folder(folder_path)"
      ],
      "metadata": {
        "id": "K_xFw_EDvpBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check if GPU is available\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")\n",
        "    print(\"Using GPU:\", torch.cuda.get_device_name(0))\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"GPU not available, using CPU.\")"
      ],
      "metadata": {
        "id": "xp8eXiUPv7yR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer\n",
        "model = SentenceTransformer('dangvantuan/french-document-embedding', trust_remote_code=True, device=device)\n",
        "embeddings = []\n",
        "for chunk in chunks:\n",
        "  sentences = chunk['content']\n",
        "  embedding = model.encode(sentences,device=device) # Use GPU for encoding)\n",
        "  embeddings.append(embedding)"
      ],
      "metadata": {
        "id": "oOTWgJ8jwC04"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convertir les embeddings en NumPy float32 pour FAISS\n",
        "import numpy as np\n",
        "embeddings = np.array(embeddings, dtype=\"float32\")\n",
        "\n",
        "import faiss\n",
        "\n",
        "# Définir la dimension des vecteurs\n",
        "dimension = embeddings.shape[1]\n",
        "\n",
        "# Créer un index FAISS avec distance L2\n",
        "index = faiss.IndexFlatL2(dimension)\n",
        "\n",
        "# Ajouter les embeddings dans l'index\n",
        "index.add(embeddings)\n",
        "\n",
        "# Exemple de métadonnées associées à chaque chunk\n",
        "#metadata = [{\"id\": i, \"content\": chunk} for i, chunk in enumerate(chunks)]\n",
        "metadata = [{\"id\": i, \"content\": chunk['content'], \"metadata\": chunk[\"metadata\"]} for i, chunk in enumerate(chunks)]\n",
        "#metadata = [item[\"metadata\"] for item in chunks]\n",
        "\n",
        "# Sauvegarder les métadonnées parallèlement\n",
        "metadata_store = {i: meta for i, meta in enumerate(metadata)}\n",
        "\n",
        "print(f\"Nombre total de vecteurs dans l'index : {index.ntotal}\")\n",
        "\n",
        "# Associer les indices aux chunks\n",
        "id_to_chunk = {i: chunk for i, chunk in enumerate(chunks)}"
      ],
      "metadata": {
        "id": "LZ7JvyLmwJLb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "vector_store = FAISS(\n",
        "    embedding_function=None,\n",
        "    index=index,\n",
        "    docstore=chunks,\n",
        "    index_to_docstore_id=id_to_chunk\n",
        "    )"
      ],
      "metadata": {
        "id": "Grw03vYHwe5H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 6. Effectuer une recherche\n",
        "\n",
        "query_vector = \"quel sont les codes de travail en benin\"\n",
        "\n",
        "# Convert the query to an embedding using the SentenceTransformer model\n",
        "query_embedding = model.encode(query_vector)\n",
        "\n",
        "# Reshape to a 2D array with a single row\n",
        "query_embedding = query_embedding.reshape(1, -1).astype(\"float32\")\n",
        "\n",
        "# Use query_embedding instead of query_vector for search\n",
        "distances, indices = index.search(query_embedding, 1)  # Trouver le plus proche\n",
        "\n",
        "# Get the index of the closest chunk\n",
        "closest_chunk_index = indices[0][0]\n",
        "\n",
        "# Access the metadata using the index from metadata_store\n",
        "result = metadata_store[closest_chunk_index]"
      ],
      "metadata": {
        "id": "P9DuLSDiwgOc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "T9xYm0qSwuDR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}